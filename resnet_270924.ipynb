{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98338d0-5eac-435d-962a-a210ce457451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84eed148-a7e0-4205-b53d-5710d611d6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27437 images belonging to 7 classes.\n",
      "Found 2238 images belonging to 7 classes.\n",
      "7\n",
      "Epoch 1/25\n",
      "  33/1524 [..............................] - ETA: 7:42 - loss: 4.6422 - accuracy: 0.3350"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Apkr\\anaconda3\\envs\\tf\\lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1524/1524 [==============================] - 560s 366ms/step - loss: 1.6953 - accuracy: 0.4509 - val_loss: 1.8506 - val_accuracy: 0.2056\n",
      "Epoch 2/25\n",
      "1524/1524 [==============================] - 604s 396ms/step - loss: 1.6145 - accuracy: 0.4591 - val_loss: 1.9750 - val_accuracy: 0.2164\n",
      "Epoch 3/25\n",
      "1524/1524 [==============================] - 614s 403ms/step - loss: 1.6093 - accuracy: 0.4618 - val_loss: 1.9655 - val_accuracy: 0.2249\n",
      "Epoch 4/25\n",
      "1524/1524 [==============================] - 1250s 820ms/step - loss: 1.5974 - accuracy: 0.4652 - val_loss: 1.9262 - val_accuracy: 0.2267\n",
      "Epoch 5/25\n",
      "1524/1524 [==============================] - 1444s 947ms/step - loss: 1.5862 - accuracy: 0.4703 - val_loss: 1.7926 - val_accuracy: 0.3087\n",
      "Epoch 6/25\n",
      "1524/1524 [==============================] - 782s 513ms/step - loss: 1.5735 - accuracy: 0.4790 - val_loss: 1.8290 - val_accuracy: 0.2572\n",
      "Epoch 7/25\n",
      "1524/1524 [==============================] - 719s 472ms/step - loss: 1.5678 - accuracy: 0.4804 - val_loss: 1.8062 - val_accuracy: 0.2603\n",
      "Epoch 8/25\n",
      "1524/1524 [==============================] - 719s 472ms/step - loss: 1.5655 - accuracy: 0.4799 - val_loss: 1.8101 - val_accuracy: 0.2572\n",
      "Epoch 9/25\n",
      "1524/1524 [==============================] - 718s 471ms/step - loss: 1.5612 - accuracy: 0.4826 - val_loss: 1.7766 - val_accuracy: 0.2984\n",
      "Epoch 10/25\n",
      "1524/1524 [==============================] - 717s 470ms/step - loss: 1.5563 - accuracy: 0.4866 - val_loss: 1.7658 - val_accuracy: 0.2975\n",
      "Epoch 11/25\n",
      "1524/1524 [==============================] - 718s 471ms/step - loss: 1.5552 - accuracy: 0.4857 - val_loss: 1.7814 - val_accuracy: 0.2796\n",
      "Epoch 12/25\n",
      "1524/1524 [==============================] - 726s 477ms/step - loss: 1.5521 - accuracy: 0.4890 - val_loss: 1.8037 - val_accuracy: 0.2737\n",
      "Epoch 13/25\n",
      "1524/1524 [==============================] - 730s 479ms/step - loss: 1.5547 - accuracy: 0.4872 - val_loss: 1.8433 - val_accuracy: 0.2536\n",
      "Epoch 14/25\n",
      "1524/1524 [==============================] - 733s 481ms/step - loss: 1.5531 - accuracy: 0.4880 - val_loss: 1.9168 - val_accuracy: 0.2312\n",
      "Epoch 15/25\n",
      "1524/1524 [==============================] - 723s 474ms/step - loss: 1.5515 - accuracy: 0.4889 - val_loss: 1.8290 - val_accuracy: 0.2594\n",
      "Epoch 16/25\n",
      "1524/1524 [==============================] - 720s 472ms/step - loss: 1.5481 - accuracy: 0.4902 - val_loss: 1.8166 - val_accuracy: 0.2661\n",
      "Epoch 17/25\n",
      "1524/1524 [==============================] - 720s 472ms/step - loss: 1.5560 - accuracy: 0.4875 - val_loss: 1.8010 - val_accuracy: 0.2688\n",
      "Epoch 18/25\n",
      "1524/1524 [==============================] - 721s 473ms/step - loss: 1.5486 - accuracy: 0.4908 - val_loss: 1.7194 - val_accuracy: 0.3495\n",
      "Epoch 19/25\n",
      "1524/1524 [==============================] - 723s 474ms/step - loss: 1.5509 - accuracy: 0.4909 - val_loss: 1.7628 - val_accuracy: 0.2935\n",
      "Epoch 20/25\n",
      "1524/1524 [==============================] - 716s 470ms/step - loss: 1.5483 - accuracy: 0.4910 - val_loss: 1.8407 - val_accuracy: 0.2513\n",
      "Epoch 21/25\n",
      "1524/1524 [==============================] - 716s 470ms/step - loss: 1.5426 - accuracy: 0.4935 - val_loss: 1.7417 - val_accuracy: 0.3253\n",
      "Epoch 22/25\n",
      "1524/1524 [==============================] - 716s 470ms/step - loss: 1.5392 - accuracy: 0.4947 - val_loss: 1.7358 - val_accuracy: 0.3221\n",
      "Epoch 23/25\n",
      "1524/1524 [==============================] - 719s 471ms/step - loss: 1.5477 - accuracy: 0.4919 - val_loss: 1.7770 - val_accuracy: 0.2867\n",
      "Epoch 24/25\n",
      "1524/1524 [==============================] - 716s 470ms/step - loss: 1.5444 - accuracy: 0.4936 - val_loss: 1.8258 - val_accuracy: 0.2558\n",
      "Epoch 25/25\n",
      "1524/1524 [==============================] - 721s 473ms/step - loss: 1.5407 - accuracy: 0.4949 - val_loss: 1.7518 - val_accuracy: 0.3091\n",
      "124/124 [==============================] - 54s 435ms/step - loss: 1.7509 - accuracy: 0.3096\n",
      "Validation Accuracy: 30.96%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet121\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Assuming your data is organized in two folders: train and validation\n",
    "train_data_dir = 'train'\n",
    "validation_data_dir = 'val'\n",
    "img_width, img_height = 150, 150  # Adjust dimensions as needed\n",
    "batch_size = 18\n",
    "epochs = 25\n",
    "\n",
    "# Choose a CNN architecture\n",
    "# Data augmentation and normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "# ResNet50\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Freeze convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "num_classes = train_generator.num_classes \n",
    "print(num_classes)\n",
    "\n",
    "# Add classification layers on top\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "hist=model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size)\n",
    "model.save(\"resnet1.keras\")\n",
    "\n",
    "loss, accuracy = model.evaluate(validation_generator, steps=validation_generator.samples // batch_size)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8eaab6-c7f9-4e22-a0ba-326bdd93dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, Label, Button, Frame, ttk\n",
    "from PIL import Image, ImageTk\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "class DeepLearningModel:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = load_model(model_path)\n",
    "    \n",
    "    def load_and_preprocess_image(self, img_path, target_size=(150, 150)):\n",
    "        img = image.load_img(img_path, target_size=target_size)\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        return img_array\n",
    "    \n",
    "    def predict(self, img_path):\n",
    "        img_array = self.load_and_preprocess_image(img_path)\n",
    "        predictions = self.model.predict(img_array)\n",
    "        predicted_class = np.argmax(predictions, axis=1)\n",
    "        return predicted_class[0]\n",
    "\n",
    "class ImageClassifierUI:\n",
    "    def __init__(self, root, model):\n",
    "        self.root = root\n",
    "        self.model = model\n",
    "        self.root.title(\"Image Classification with Keras\")\n",
    "        self.root.geometry(\"500x400\")\n",
    "\n",
    "        # Create a main frame\n",
    "        self.main_frame = Frame(root, bg=\"#f0f0f0\")\n",
    "        self.main_frame.pack(fill=\"both\", expand=True, padx=20, pady=20)\n",
    "\n",
    "        # Title\n",
    "        self.title_label = Label(self.main_frame, text=\"Image Classification\", font=(\"Helvetica\", 18, \"bold\"), bg=\"#f0f0f0\")\n",
    "        self.title_label.pack(pady=10)\n",
    "\n",
    "        # Image display area\n",
    "        self.img_frame = Frame(self.main_frame, bg=\"#ffffff\", borderwidth=2, relief=\"sunken\")\n",
    "        self.img_frame.pack(pady=10)\n",
    "        self.img_label = Label(self.img_frame)\n",
    "        self.img_label.pack()\n",
    "\n",
    "        # Load button\n",
    "        self.load_button = Button(self.main_frame, text=\"Load Image\", command=self.load_image, bg=\"#4CAF50\", fg=\"white\", font=(\"Helvetica\", 12))\n",
    "        self.load_button.pack(pady=10)\n",
    "\n",
    "        # Result label\n",
    "        self.result_label = Label(self.main_frame, text=\"\", font=(\"Helvetica\", 14), bg=\"#f0f0f0\")\n",
    "        self.result_label.pack(pady=10)\n",
    "\n",
    "        # Progress bar\n",
    "        self.progress = ttk.Progressbar(self.main_frame, mode='indeterminate')\n",
    "        self.progress.pack(pady=10)\n",
    "        self.progress_text = Label(self.main_frame, text=\"\", font=(\"Helvetica\", 12), bg=\"#f0f0f0\")\n",
    "        self.progress_text.pack(pady=5)\n",
    "\n",
    "        # To keep reference to image\n",
    "        self.img_ref = None\n",
    "\n",
    "    def load_image(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Image Files\", \"*.jpg;*.jpeg;*.png\")])\n",
    "        if file_path:\n",
    "            self.show_image(file_path)\n",
    "            self.predict_image(file_path)\n",
    "    \n",
    "    def show_image(self, file_path):\n",
    "        img = Image.open(file_path)\n",
    "        img.thumbnail((300, 300))\n",
    "        img = ImageTk.PhotoImage(img)\n",
    "\n",
    "        # Update image label\n",
    "        self.img_label.config(image=img)\n",
    "        self.img_label.image = img\n",
    "\n",
    "        # Keep reference to the image object\n",
    "        self.img_ref = img\n",
    "\n",
    "    def predict_image(self, file_path):\n",
    "        self.progress.start()\n",
    "        self.progress_text.config(text=\"Processing...\")\n",
    "        self.root.update_idletasks()\n",
    "        \n",
    "        predicted_class = self.model.predict(file_path)\n",
    "\n",
    "        # Simulate some processing delay\n",
    "        self.root.after(2000, lambda: self.update_result(predicted_class))\n",
    "\n",
    "    def update_result(self, predicted_class):\n",
    "        self.progress.stop()\n",
    "        self.progress_text.config(text=\"Done!\")\n",
    "        self.result_label.config(text=f'Predicted class: {predicted_class}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the deep learning model with the path to your model\n",
    "    deep_learning_model = DeepLearningModel('resnet_model.h5')\n",
    "\n",
    "    # Initialize the Tkinter root window\n",
    "    root = tk.Tk()\n",
    "\n",
    "    # Initialize the UI with the root window and the deep learning model\n",
    "    app = ImageClassifierUI(root, deep_learning_model)\n",
    "\n",
    "    # Start the Tkinter event loop\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
